{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Basics: Maximum Likelihood Estimation\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "This is the first of a set of little blog-style post that I'm creating to get a better grasp on machine learning concepts. I'm mainly following the book \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio and Aaron Courville [1], but if any other resources are used I'll be citing them underneath this introduction. While alot of these examples are going to be ones I take from [1] I think sometimes it helps to provide some context or explanation to an equation which is what I'm going to try to do throughought this series. If you see any mistakes please feel free to email me at adibfixeshismistakes@gmail.com.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Refernces\n",
    "https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1 [2] (Jonny Brooks-Bartlett)\n",
    "\n",
    "https://www.probabilitycourse.com/chapter8/8_2_0_point_estimation.php [3] (Hossein Pishro-Nik)\n",
    "\n",
    "https://towardsdatascience.com/mse-and-bias-variance-decomposition-77449dd2ff55 [4] (Maksym Zavershynskyi)\n",
    "\n",
    "https://www.math.ubc.ca/~pwalls/math-python/integration/riemann-sums/ [5] (Patrick Walls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimators\n",
    "\n",
    "Estimators are functions that can be used to provide the best possible estimate ($\\hat{\\theta}$) of some quantity of interest (${\\theta}$) where the true value $\\theta$ is some fixed quantity for the distribution. If you're thinking \"wow this is a very vague definition\", you're right, it is! If {$x^{(1)}, x^{(2)},...,x^{(m)}$} are a set of independent, identically distributed data points collected by sampling some random variable $X$. The *point estimator* is some function g such that:\n",
    "\n",
    "$$\\hat{\\theta} = g(x^{(1)}, x^{(2)}, ... x^{(m)})$$\n",
    "\n",
    "which means pretty much any function can be considered an estimator. If you are sampling some random variable ${X}$ with an unknown parametric probability density we can estimate the parameters of the model by making some educated guesses about the type of distribution the sample data best resembles. \n",
    "\n",
    "Using an example from [1], suppose we have our set of samples and they are distributed according to some gaussian distribution with unknown parameters $\\mu$ and $\\sigma^2$. We have:\n",
    "\n",
    "$$ P(x^{(i)}; \\mu; \\sigma^2) = N(x^{(i)}; \\mu; \\sigma^2) $$\n",
    "\n",
    "$$ P(x^{(i)}; \\mu; \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp^{-\\frac{(x^{(i)} - \\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "We dont know $\\mu$ or $\\sigma^2$, just that the sampled data may be modelled by a gaussian distribution, but we can estimate $\\hat{\\mu}$ by just taking the average value of all of the sampled points.\n",
    "\n",
    "$$\\hat{\\mu} = \\frac{1}{m}\\sum^{m}_{i=1}x^{(i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Estimators\n",
    "\n",
    "Notice that while this example has a pretty reasonable estimator for $\\hat{\\mu}$, the definition of an estimator makes no guarantees that the estimator will accurately predict the value that its trying to estimate. So we need some measure of how well an estimator will perform, or more importantly how closely it will come to the true value of $\\theta$. The *bias* and *variance* of an estimator are measures of its offset from the true value of $\\theta$ and how much it will vary as we apply the estimator to multiple independently sampled data sets. They are defined as:\n",
    "\n",
    "$$ Bias(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta $$\n",
    "\n",
    "$$ Variance = Var(\\hat{\\theta})$$\n",
    "\n",
    "Continuing the example from before, the bias for our estimator of $\\hat{\\mu}$ can be calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Bias(\\hat{\\mu}) &= E[\\hat{\\mu}] - \\mu \\\\\n",
    "                &= E[\\frac{1}{m}\\sum^{m}_{i=1}x^{(i)}] - \\mu \\\\\n",
    "                &= \\frac{1}{m}\\sum^{m}_{i=1}E[x^{(i)}] - \\mu \\\\\n",
    "                &= \\frac{1}{m}\\sum^{m}_{i=1}\\mu - \\mu \\\\\n",
    "                &= \\mu - \\mu \\\\\n",
    "                &= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This shows that using the sample mean as an estimate for the gaussian mean parameter results in an unbiased estimator.\n",
    "\n",
    "Similarly we can try to calculate the variance of the estimator.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Var(\\hat{\\mu}) &= Var(\\frac{1}{m}\\sum^{m}_{i=1}x^{(i)}) \\\\\n",
    "               &= \\frac{1}{m}Var(\\sum^{m}_{i=1}x^{(i)}) \\\\\n",
    "               &= \\frac{1}{m}\\sum^{m}_{i=1}Var(x^{(i)}) \\\\\n",
    "               &= \\sigma^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "While these metrics are useful ultimately what we want to do when choosing between estimators is to pick the one with the lowest amount of error between $\\hat{\\theta}$ and $\\theta$. Mean squared error does exactly this, and is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "MSE(\\hat{\\theta}) &= E[(\\hat{\\theta} - \\theta)^2] \\\\\n",
    "                  &= E[(\\hat{\\theta}^2 - 2 \\hat{\\theta}\\theta + \\theta^2] \\\\\n",
    "                  &= E[(\\hat{\\theta}^2] - 2E[\\hat{\\theta}\\theta] + E[\\theta^2]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "I decomposed the equation a little bit to show you the dependence of the MSE on the bias and variance of an estimator. To complete this derivation we need to work out two other derivations.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Bias(\\hat{\\theta})^2 &= (E[\\hat{\\theta}] - \\theta)^2 \\\\\n",
    "                     &= (E[\\hat{\\theta}])^2 - 2E[\\hat{\\theta}\\theta] + E[\\theta^2] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We assume that the real value $\\theta$ isn't a random variable, so its expectation is equal to its value. This leaves us with:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Bias(\\hat{\\theta})^2 &= (E[\\hat{\\theta}])^2 - 2\\theta E[\\hat{\\theta}] + \\theta^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The second derivation we need is a decomposition of the variance of an estimator.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Var(\\hat{\\theta}) &= E[(\\hat{\\theta} - E[\\hat{\\theta}])^2] \\\\\n",
    "                  &= E[(\\hat{\\theta}^2 - 2\\hat{\\theta}E[\\hat{\\theta}] + (E[\\hat{\\theta}])^2] \\\\\n",
    "                  &= E[(\\hat{\\theta}^2] - 2E[\\hat{\\theta}E[\\hat{\\theta}]] + E[(E[\\hat{\\theta}])^2] \\\\\n",
    "                  &= E[(\\hat{\\theta}^2] - 2E[\\hat{\\theta}]^2 + E[\\hat{\\theta}]^2 \\\\\n",
    "                  &= E[(\\hat{\\theta}^2] - E[\\hat{\\theta}]^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "A potentially non-obvious trick that is used in the derivation above is that $E[E[x]]$ is actually taking the expected value of a scalar non-random variable, so its equal to $E[x]$. This is how we are able to convert $2E[\\hat{\\theta}E[\\hat{\\theta}]] = 2E[\\hat{\\theta}]^2$ and likewise, $E[(E[\\hat{\\theta}])^2] = E[\\hat{\\theta}]^2$.\n",
    "\n",
    "Now finally, if we put these two derivations together:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Var(\\hat{\\theta}) + Bias(\\hat{\\theta})^2 &= E[(\\hat{\\theta}^2] - E[\\hat{\\theta}]^2 + (E[\\hat{\\theta}])^2 - 2E[\\hat{\\theta}\\theta] + E[\\theta^2] \\\\\n",
    "                                        &= E[(\\hat{\\theta}^2] + 2E[\\hat{\\theta}\\theta] + E[\\theta^2] \\\\\n",
    "                                        &= MSE(\\hat{\\theta})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This might seem like a long walk for a small drink of water but this derivation of the bias-variance decomposition of the MSE shows that when comparing the viability of two seperate estimators we dont really care about the variance or bias independently, but rather the balance between then that achieves the lowest MSE (obviously ideally we want both of them to be low!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimators in Machine Learning\n",
    "So far I've only been talking about point estimators, but as mentionned earlier an estimator can also be a function. In this case instead of trying to estimate the value of some quantity $\\theta$ we are trying to estimate the relationship between two quantities. In this scenario we want to generate a function estimator on a training set of I.I.D data, and then use this estimator on a completely different set (the test data) to try and accurately predict. Suppose our desired output quantity is $y$ and we have a set of I.I.D data samples {$x^{(1)}, x^{(2)}..., x^{(m)}$} in some training dataset S. We want to find an estimate $\\hat{f}_S$ of some function f such that:\n",
    "\n",
    "$$\n",
    "y = f(x) + \\epsilon\n",
    "$$\n",
    "\n",
    "Where $\\epsilon$ represents all of the noise components of y that cannot be estimated from just $x$ alone with some probability distribution that is different from $S$. In this scenario we want to calculate MSE as:\n",
    "$$\n",
    "MSE = E[(y - \\hat{f}_S(x))^2]\n",
    "$$\n",
    "\n",
    "We can decompose this equation into some key components but we need to utilize some key identities of the variance and expected value of two random variables. The derivation for these identities are going to be omitted, since they're widely available online. Note both a and b in the identities below are random variables. These identities are identical to those presented in [4].\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Var(a) &= E[a^2] - E^2[a]\\\\\n",
    "E[ab] &= E[a]E[b] + Cov(a,b) \\\\\n",
    "Var(a + b) &= Var(a) + Var(b) + 2Cov(a,b) \\\\\n",
    "Var(a - b) &= Var(a) + var(b) - 2Cov(a,b) \\\\\n",
    "Cov(a, b) &= 0\\ when\\ a\\ and\\ b\\ are\\ independent\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we can break down $MSE[\\hat{f(x)}]$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "MSE[\\hat{f(x)}] &= E[(y - \\hat{f(x)})^2] \\\\\n",
    "&= E[y^2] - 2E[y\\hat{f(x)}] + E[\\hat{f(x)}^2] \\\\\n",
    "&= Var(y) + E[y]^2 + Var(\\hat{f(x)}) + E[\\hat{f(x)}]^2 -2E[y\\hat{f(x)}] \\\\\n",
    "&= Var(y) + E[y]^2 + Var(\\hat{f(x)}) + E[\\hat{f(x)}]^2 -2E[(f(x) + \\epsilon)\\hat{f(x)}] \\\\\n",
    "&= Var(y) + E[y]^2 + Var(\\hat{f(x)}) + E[\\hat{f(x)}]^2 -2E[(f(x))(\\hat{f(x))}] -2E[\\epsilon\\hat{f(x)}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can now take advantage of the fact that $y = f(x) + \\epsilon$. This has already been used to simplify the $-2E[y\\hat{f(x)}]$ term, but now we can apply it to get a variance and bias term in terms of $f(x)$ and $\\hat{f(x)$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&= Var(f(x) + \\epsilon) + E[f(x) + \\epsilon]^2 + Var(\\hat{f(x)}) + E[\\hat{f(x)}]^2 -2E[(f(x))]E[(\\hat{f(x))}] - 2Cov(f(x), \\hat{f(x)}) -2E[\\epsilon\\hat{f(x)}] \\\\\n",
    "&= Var(f(x)) + Var(\\epsilon) - 2Cov(f(x), \\epsilon) + E[f(x)]^2 + 2E[\\epsilon]E[f(x)] + 2E[\\epsilon]^2 + Var(\\hat{f(x)}) + E[\\hat{f(x)}]^2 -2E[(f(x))]E[(\\hat{f(x))}] + Cov(f(x), \\hat{f(x)}) -2E[\\epsilon\\hat{f(x)}] \\\\\n",
    "&= Var(f(x) - \\hat{f(x)}) + E[f(x)]^2 - 2E[f(x)]E[\\hat{f(x)}] + E[\\hat{f(x)}]^2 + Var(\\epsilon) - 2Cov(f(x), \\epsilon) + 2E[\\epsilon]E[f(x)] + 2E[\\epsilon]^2 - 2E[\\epsilon\\hat{f(x)}]\\\\\n",
    "&= Var(f(x) - \\hat{f(x)}) + (E[f(x)] - E[\\hat{f(x)}])^2 + Var(\\epsilon) - 2Cov(f(x), \\epsilon) + 2E[\\epsilon]E[f(x)] + 2E[\\epsilon]^2 - 2E[\\epsilon\\hat{f(x)}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can simplify this a little further by assuming that the noise term $\\epsilon$ is independent of our data set $S$, and that it has some constant mean $c$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&= Var(f(x) - \\hat{f(x)}) + (E[f(x)] - E[\\hat{f(x)}])^2 + Var(\\epsilon) - 2cE[f(x)] + 2c^2 - 2cE[\\hat{f(x)}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This can be simplified further still if we assume our noise model has mean $c=0$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&= Var(f(x) - \\hat{f(x)}) + (E[f(x)] - E[\\hat{f(x)}])^2 + Var(\\epsilon)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We're left with an equation that is very similar to the bias-variance decomposition of the MSE for point estimators. The variance term is $Var(f(x) - \\hat{f(x)})$, measuring the amount of jitter we can expect the difference between the true model f(x) and $\\hat{f(x)}$ to be as we vary $S$. The bias term is $(E[f(x)] - E[\\hat{f(x)}])^2$, which is a measure of how well the estimator model will approximate the real model. Finally we have an additional term $Var(\\epsilon)$ which is a measure of how much the noise term will impact the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Likelihood Estimation\n",
    "So far most of the analysis in this post has been about evaluating estimators, but no explanation has been given as to how we actually generate these estimators. Maximum likelihood estimation is a very simple algorithm for generating an estimator for parameter values of some data model. It states that we should pick the value of $\\hat{\\theta}$ that gives us the highest probability of observing our training data. Mathematically we can define this as the optimization of the maximum of some likelihood function $L$ operating on the training data {$x_1,x_2,...x_m$} and the unknown parameter values $\\hat{\\theta}$. We need to make a differentiation between $X_1$, which is the first sampling of random variable $X$ and $x_1$ which is the outcome of the first sampling of random variable $X$.\n",
    "\n",
    "$$\n",
    "L(x_{(1)},x_{(2)},...x_{(m)}; \\hat{\\theta}) = P(X_1 = x_1,X_2 = x_2,X_m = x_m;\\hat{\\theta})\n",
    "$$\n",
    "\n",
    "Where $P$ is the probability distribution (if X is a continuous random variable) or probability mass function (if X is a discrete random varaible). Now the maximum likelihood estimate $\\hat{\\theta}_{ML}$ is the value of $\\hat{\\theta}$ that maximizes $L(x_1,x_2,...x_m;\\hat{\\theta}$. We can also do this for a vector of parameters $\\hat{\\theta}_1,\\hat{\\theta}_2,...\\hat{\\theta}_m$ rather than a single value, adapting from using it as a point estimator to a function estimator instead.\n",
    "\n",
    "Lets look at some simple examples. Consider a set of data modeled by random variable $X$ with a normal distribution $N(X; \\mu, \\sigma^2)$. Suppose we want to estimate the mean $\\mu$ and variance $\\sigma^2$ of $X$ using a sample data set {$x_1,x_2,...x_m$}.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(X; \\mu, \\sigma^2) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp^{-\\frac{(X - \\mu)^2}{2\\sigma^2}} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard method for finding the maximum of a function is to look at its first derivative with respect to the variable that we're trying to optimize. Since we have two unknowns in this situation we can do this by taking the partial derivatives $\\frac{dL}{d\\mu}$ and $\\frac{dL}{d\\sigma^2}$. Before differentiating, its a good idea to take the natural log of $L$ to make the differentiation process easier. We're able to do this because the logarithmic function is a montonic increasing function the peak of our exponential function and the peak of its natural log will be at the same spot, and we are only interested in the location of the peak during for optimization. Now, assuming that each sample is I.I.D we can find the partial derivatives.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ln{L(x_1,x_2,...x_m; \\mu, \\sigma^2)} &= \\ln{\\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}\\exp^{-\\frac{\\sum^{m}_{i=1}(x_i - \\mu)^2}{2\\sigma^2}}}\\\\\n",
    "&= ln{\\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}} - \\frac{1}{2\\sigma^2}\\sum^{m}_{i=1}(x - \\mu)^2\\\\\n",
    "&= -\\ln{(2\\pi\\sigma^2)^{\\frac{n}{2}}} - \\frac{1}{2\\sigma^2}\\sum^{m}_{i=1}(x - \\mu)^2\\\\\n",
    "&= -\\frac{n}{2}\\ln{2\\pi\\sigma^2} - \\frac{1}{2\\sigma^2}\\sum^{m}_{i=1}(x - \\mu)^2\\\\\n",
    "&= -\\frac{n}{2}\\ln{2\\pi} - \\frac{n}{2}\\ln{\\sigma^2} - \\frac{1}{2\\sigma^2}\\sum^{m}_{i=1}(x - \\mu)^2\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{dL}{d\\mu} &= \\frac{1}{\\sigma^2}\\sum^{m}_{i=1}(x_i - \\mu)\\\\\n",
    "\\frac{dL}{d\\sigma^2} &= -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}\\sum^m_{i=1}(x_i - \\mu)^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the maxima, we have to set $\\frac{dL}{d\\mu}$ and $\\frac{dL}{d\\sigma^2}$ to 0 and solve for $\\mu$ and $\\sigma^2$ respectively.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "0 &= \\frac{dL}{d\\mu}\\\\\n",
    "0 &= \\frac{1}{\\sigma^2}\\sum^{m}_{i=1}(x_i - \\mu) \\\\\n",
    "0 &= \\sum^{m}_{i=1}(x_i - \\mu) \\\\\n",
    "0 &= \\sum^{m}_{i=1}x_i - \\sum^{m}_{i=1}\\mu \\\\\n",
    "0 &= \\sum^{m}_{i=1}x_i - m\\mu\\\\\n",
    "\\mu &= \\frac{1}{m}\\sum^{m}_{i=1}x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "Interestingly, MLE converges on the sample mean as the estimator for the gaussian mean, and from the estimator evaluaation section we know that the sample mean is an unbiased estimator of the gaussian mean so the mean-squared error is likely to be low.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "0 &= -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}\\sum^m_{i=1}(x_i - \\mu)^2\\\\\n",
    "0 &= -\\frac{n\\sigma^2}{2} + \\sum^m_{i=1}(x_i - \\mu)^2\\\\\n",
    "\\sigma^2 &= \\frac{2}{n}\\sum^m_{i=1}(x_i - \\mu)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "Likewise MLE converges on an equation that is very close to the sample variance as the optimal estimator for the gaussian variance. Lets look at this working in code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets generate a gaussian function to model our probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9980994203824746\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def gaussian(mean, variance, x):\n",
    "    '''Samples a gaussian distribution with specified mean and variance'''\n",
    "    return (1 / np.sqrt((2 * np.pi * variance))) * np.exp(-0.5 * (((x - mean)**2)/(variance)))\n",
    "\n",
    "def integrate(f, xaxis, dx):\n",
    "    '''Calculate the midpoint Riemann sum integral approximation for function f'''\n",
    "    mid_points = (xaxis[:-1] + xaxis[1:]) / 2\n",
    "    return mid_points, np.cumsum(f(mid_points)) * dx\n",
    "\n",
    "def gaussian_sampling(num_samples, mean, variance):\n",
    "    '''Generates an array of random gaussian distributed points with a parametric distribution\n",
    "       defined by mean and variance'''\n",
    "    samples = np.random.rand(num_samples)\n",
    "    xmin = 0.05 * mean\n",
    "    xmax = 2.05 * mean\n",
    "    linear_space = np.linspace(xmin, xmax, num_samples)\n",
    "    dx = (xmax - xmin) / num_samples\n",
    "    integral_x, integral_y = integrate(functools.partial(gaussian, mean, variance), linear_space, dx)\n",
    "    points = []\n",
    "    for sample in samples:\n",
    "        try:\n",
    "            points.append(integral_x[(np.where(integral_y >= sample))[0][0]])\n",
    "        except IndexError:\n",
    "            # The randomly generated sample is too close to 1 for us to produce a result with\n",
    "            # the given number of samples. Ignore it and move on with life.\n",
    "            continue\n",
    "    return points\n",
    "\n",
    "\n",
    "GROUND_TRUTH_MEAN = 20\n",
    "GROUND_TRUTH_VARIANCE = 2\n",
    "X_MIN = 0\n",
    "X_MAX = 40\n",
    "N_SAMPLE = 150\n",
    "dx = (X_MAX - X_MIN) / N_SAMPLE\n",
    "\n",
    "linear_sample = np.linspace(X_MIN, X_MAX, N_SAMPLE)\n",
    "\n",
    "fig = plt.figure()\n",
    "pdf = plt.subplot(1,3,1)\n",
    "pdf.set_xlim(X_MIN, X_MAX)\n",
    "pdf.set_title(\"Probability Distribution\")\n",
    "\n",
    "cdf = plt.subplot(1,3,2)\n",
    "cdf.set_xlim(X_MIN, X_MAX)\n",
    "cdf.set_ylim(0, 1.5)\n",
    "cdf.set_title(\"Cumulative Distribution\")\n",
    "\n",
    "gaussian_random_samples = plt.subplot(1,3,3)\n",
    "gaussian_random_samples.set_xlim(GROUND_TRUTH_MEAN  - 5 * GROUND_TRUTH_VARIANCE, GROUND_TRUTH_MEAN  + 5 * GROUND_TRUTH_VARIANCE)\n",
    "gaussian_random_samples.set_ylim(-0.5 , 0.5)\n",
    "gaussian_random_samples.set_title(\"Gaussian Random Samples\")\n",
    "plt.subplots_adjust(bottom=0.1, right=1.8, top=0.9)\n",
    "\n",
    "pdf.plot(linear_sample,\n",
    "         gaussian(GROUND_TRUTH_MEAN, GROUND_TRUTH_VARIANCE, linear_sample),\n",
    "         label=\"Gaussian Probability Distribution\")\n",
    "\n",
    "data_dist = functools.partial(gaussian, GROUND_TRUTH_MEAN, GROUND_TRUTH_VARIANCE)\n",
    "integral_x, integral_y = integrate(data_dist, linear_sample, dx)\n",
    "cdf.plot(integral_x, integral_y)\n",
    "cdf.bar(integral_x, data_dist(integral_x), width=(X_MAX - X_MIN)/ N_SAMPLE, alpha=0.2, edgecolor='b')\n",
    "\n",
    "samples = np.array(gaussian_sampling(200, GROUND_TRUTH_MEAN, GROUND_TRUTH_VARIANCE))\n",
    "gaussian_random_samples.scatter(samples, np.zeros(samples.size), s=2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Above we have a gaussian probability distribution function that we've defined with $\\mu$ = GROUND_TRUTH_MEAN and\n",
    "\n",
    "$\\sigma^2$ = GROUND_TRUTH_VARIANCE. The parameters are defined as \"ground truth\" because they are the true parametric values of the data model. Now we'll try to use maximum likelihood estimation to try and get a good estimate $\\hat{\\mu}$ and $\\hat{\\sigma^2}$ from our gaussian data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
