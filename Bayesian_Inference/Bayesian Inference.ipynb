{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Basics: Bayesian Inference\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "This is the second of a set of little blog-style post that I'm creating to get a better grasp on machine learning concepts. I'm mainly following the book \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio and Aaron Courville [1], but if any other resources are used I'll be citing them underneath this introduction. While alot of these examples are going to be ones I take from [1] I think sometimes it helps to provide some context or explanation to an equation which is what I'm going to try to do throughought this series. If you see any mistakes please feel free to email me at adibfixeshismistakes@gmail.com.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional References\n",
    "https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/ [2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequentist vs Bayesian statistics\n",
    "\n",
    "Frequentist statistics assign probabilities to repeatable random events. The probabilities of these events are equal to their long-term frequency of occurance. With that definition one-off events can't be assigned a probability since we dont actually have any idea how often they will occur from just one occurance. Bayesian statistics try to use probabilities to represent the amount of uncertainty in a given event.\n",
    "\n",
    "Suppose we take maximum likelihood estimation of some parameter in a probability model as an example. The frequentist approach is to assume that the parameter we're trying to estimate has some fixed value, so there cant be any probabilities assigned to the value of the parameter being equal to any given value, all we can do is to collect some data and estimate the parameter based on the value that is most likeliy to produced the training data. Alternatively the Bayesian perspective accepts that the parameter is fixed an unknown but there can still be a probability distribution defined over all the possible values of the mean. The sample data is used to tap into this distribution and pick the best estimate for the parameter. This is done using Bayes' Theorem.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(Parameter | Data) = \\frac{P(Data | Parameter) P(Parameter)}{P(Data)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$P(Parameter)$ is known as the prior probability, the assumed distribution of the parameter we're trying to estimate. $P(Data)$ is the evidence, the assumed distribution of the data being observed.  $P(Data|Parameter)$ is the likelihood of observing a given data set at a particular value of the parameter. $P(Parameter | Data)$ is known as the posterior probability its the probability that the selected value for our parameter is the true value for the data distribution. This is similar to the confidence intervals that can be generated by utilizing the frequentist approach, but instead of providing a range of values for an expected level of certainty we're extracting the amount of certainty that we can expect for a given estimate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
